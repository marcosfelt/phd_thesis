%************************************************
\chapter{DeepGamma: A deep learning model for activity coefficient prediction}\label{ch:deep_gamma} 
%************************************************
This chapter is based on the following paper:
\begin{enumerate}
\item \textbf{Felton K.}, Ben-Safar, H., and Lapkin A. \textit{AAAI Workshop on AI to Accelerate Science and Engineering}. "\textit{DeepGamma}: A deep learning model for activity coefficient prediction." 2022.
\end{enumerate}
\textbf{My contribution}: I executed the COSMO-RS calculations, trained the final version of the \textit{DeepGamma} models and wrote the manuscript.

\section{Introduction}

Vapor-liquid equilibrium thermodynamics play an essential role in a wide variety of fields in the basic and applied sciences. For example, chemical reactions often happen in between the vapor and liquid phase, where vapor produced by a reaction is extracted to drive conversion or a reactant is introduced as a vapor. Similarly, large scale chemical processes require separation of mixtures into components to purify a valuable product. One ubiquitous method for achieving such separations is distillation, which relies on differences in boiling points of mixtures to achieve separation.

In order to design and engineer systems which contain vapor-liquid equilibrium, thermodynamic equations are utilized. Thermodynamic equations describe the relationship between the composition of the liquid and the vapor at a given temperature and pressure. One well-known thermodynamic equation is Raoult's Law: 

\begin{equation}
    \label{ideal-gas}
    y_i P = x_iP_i^{sat}(T)
\end{equation}

where $y_i$ and $x_i$ are the vapor and liquid compositions of component $i$ of the mixture respectively, $P$ is the absolute pressure, and $P_i^{sat}(T)$ is the vapor pressure at temperature $T$. Raoult's law describes non-interacting ideal systems, yet it fails to properly predict vapor-liquid equilibrium the wide variety of mixtures used in industrial chemical processes. Therefore, the simplified gamma-phi equation was developed to describe deviations from ideality:

\begin{equation}
    \label{gamma-phi}
    y_i P = x_i \gamma_i(x,T,P)   \exp \biggl(\frac{V_i^L(P-P_i^{sat}(T))}{RT}\biggr)
\end{equation}

where $\gamma_i(\mathbf x,T,P)$ is the activity coefficient at liquid composition $\mathbf x$,  temperature $T$, and pressure $P$; $V_i^L$ is the specific volume and $P_i^{sat}$ is the vapor pressure.  

Activity coefficients are the unknown parameters in the gamma-phi thermodynamic equation that correct for deviations from ideality. These activity coefficients must be predicted for each new mixture using thermodynamic models such as the non-random two liquid model \cite{Renon1968}. 

Unfortunately, the vapor-liquid equilibrium experiments required to parameterize thermodynamic models for predicting activity coefficients are notoriously time-intensive to complete. Classical thermodynamic measurement apparatus such as stills require equilibrium to be reached at each temperature and pressure combination before a measurement is taken \cite{Ronc1976, Dechambre2014}. This equilibration process can take anywhere from minutes to hours, meaning a full set of experiments for a new mixture can take weeks to months. As a result, vapor liquid equilibrium experiments are executed very selectively.

To reduce the time required for parameterizing thermodynamic models for predicting activity coefficients, there is a large body of research focused on directly predicting the parameters from molecular structures. This work ranges from simple counting of functional groups \cite{Fredenslund1975} to density functional theory \cite{ Klamt2010} and machine learning \cite{Urata2002, Nami2011,  Jirasek2020}.  However, these prediction approaches often only cover a small subset of all mixture types (e.g., only hydrocarbons) or require significant calculation times. Thus, there is a need for a general, fast and accurate method for predicting activity coefficients \textit{a priori}.

In this chapter, I introduce \textit{DeepGamma}, a deep learning model for fast predictions of activity coefficients directly from molecular structures.  I leverage message passing neural networks (MPNN) to reproduce the results of quantum simulations for predicting activity coefficients without computational expense. Specifically, this model decreases the time required for calculations by a factor of 1900. In the following section, I briefly review methods for predictive thermodynamics before discussing the methodology and results.

\section{Background}

I classify existing methods for predictive thermodynamics into three categories: group-contribution methods, quantum chemistry based methods and machine learning methods. 

\subsubsection{Group Contribution Methods}

Group contribution methods such as UNIFAC were originally developed in the 1970s and predict the activity coefficient as a weighted sum of the occurrence of a predetermined set of basis functions with inputs as the chemical functional groups in each component \cite{Fredenslund1975}. UNIFAC specifically predicts the natural logarithm of the activity coefficient of component $i$ in  a mixture as the sum of a combinatorial term $\ln \gamma_i^C$ and a residual term $\ln \gamma_i^R$:

\begin{equation}
    \ln \gamma_i = \ln \gamma_i^C + \ln \gamma_i^R
\end{equation}

The combinatorial term represents the combination of different functional groups:

\begin{equation}
    \ln  \gamma^C_i = \ln \frac{\Phi_i}{x_i} + \frac{z}{2} q_i \ln \frac{\theta_i}{\Phi_i} + l_i - \frac{\Phi_i}{x_i} \sum_jx_jl_j
\end{equation}

where $\Phi_i$ and $\theta_i$ are the weighted molar segment area and area for component. These variables in addition to $l_j$ are a function of $r_i$, $z$ and $q_i$, which are  in turn functions of the group volume and surface area parameters $R_k$ and $Q_k$ for tabulated functional groups:

\begin{equation}
    \Phi_i  = \frac{r_i x_i}{\sum_j r_j x_j}
\end{equation}

\begin{equation}
    \theta_i = \frac{q_i x_i}{\sum_j q_j x_j}
\end{equation}

\begin{equation}
    l_i = \frac{z}{2}(r_i-q_i) - (r_i -1) \;\; z=10
\end{equation}

\begin{equation}
    r_i  = \sum_k = v_k^i R_k
\end{equation}

\begin{equation}
    q_i = \sum_k v_k^i Q_k
\end{equation}

$v_k^i$ is the number of groups of type $k$ in component $i$.

The residual term represents inter-group interactions. It is composed of the group residual activity coefficient $\Gamma_k$ and  $\Gamma_k^{(i)}$, the residual activity coefficient of group k in reference solution with only component $i$.

\begin{equation}
    \ln \gamma^R_i = \sum_k v_k^{(i)}(\ln \Gamma_k - \ln \Gamma_k^{(i)})
\end{equation}

\begin{equation}
    \ln \Gamma_k = Q_k \biggl[1 - \ln \bigl(\sum_m \theta_m \psi_{mk}\bigr) - \sum_m \frac{\theta_m \Psi_{km}}{\sum_n \theta_n \Psi_{nm}} \biggr]
\end{equation}

\begin{equation}
    \theta_m = \frac{Q_m X_m}{\sum_n Q_n X_n}
\end{equation}

\begin{equation}
    \Psi_{mn} = \exp{\frac{a_{mn}}{T}}
\end{equation}

where $a_{mn}$ is a binary interaction parameter. 

While UNIFAC provides fast predictions, the accuracy of the model is limited by the interactions explicitly accounted for in each basis function. It can be challenging to hand-craft all such interactions as they can often extend between several atoms.

\subsubsection{Quantum Chemistry}

The second set of methods are quantum chemistry methods. These simulations often use a combination of molecular dynamics and density functional theory (DFT) calculations to predict activity coefficients for each mixture component \cite{Constantinescu2005}. COSMO-RS is one of the most reliable computational methods for liquid-phase thermodynamic predictions \cite{Klamt1995, Klamt2010}. It relies on the theory of screening charges, which states that every element of a surface of a dissolved solute must be complemented by an opposite charge in the solvent  \cite{Klamt1995}. The charge surface around a solute is divided into infinitesimally small pieces, which are then integrated to find the charge density. This charge density can then be used to calculate activity coefficients. The charge density is calculated via rigorous DFT calculations. COSMO-RS and a related method named COSMO-SAC have been applied in calculations of VLE \cite{Constantinescu2005}, liquid-liquid equilibrium (LLE) \cite{Dechambre2014} and vapor-liquid-liquid-equilibrium (VLLE) curves \cite{Kundu2011}.

The downside of COSMO methods is that they are often not accurate for polar compounds \cite{Constantinescu2005, Kundu2011}. This is often due to the lack of theory for the hydrogen bonding present in these systems \cite{Kundu2011}. Another challenging aspect of COSMO-RS is the computational intensity of DFT calculations for new mixtures. 

\subsubsection{Machine Learning}

There has been significant work on directly predicting infinite dilution activity coefficients from molecular structures and closely related free solvation energies using simple neural networks \cite{Urata2002, RamirezBeltran2009, Nami2011, Behrooz2017}, matrix completion\cite{Jirasek2020} and  graph neural networks \cite{Vermeire2021, Felton2022, SanchezMedina2022, Qin2022, Rittig2022}. However, the limitation of these works is their lack of thermodynamic consistency and other guarantees that come with rigorously derived equations of state. To overcome this problem, recent work has attempted to predict EoS parameters \cite{Abbasi2020, Madani2021, Abdallahelhadj2022, Winter2022}, which can be in turn used to predict thermodynamic outcomes.
 
\section{Methods}
\subsection{Models}

\noindent
\textbf{Message passing neural networks:} Molecules can be treated as graphs with atoms as nodes and bonds as edges. Therefore, message passing neural networks (MPNN) that operate on graphs can be used for end-to-end prediction of molecular properties \cite{Gilmer2017}.  In a MPNN, each node passes a vector (i.e., its current features) to its neighbor. This  vector is called a message. In each layer, a permutation invariant aggregation function such as the mean is used to calculate the total message for each node prior to passing through an activation function to get a new value for each node.  In contrast to traditional fixed fingerprints like ECFP \cite{Rogers2010}, the best feature vector is learned end-to-end for each property prediction task.

One type on MPNN is a directed message passing neural network (D-MPNN) in which the encoder acts on edges (bonds) instead of nodes (atoms) to improve stability of training \cite{Yang2019}. Yang, Swanson and colleagues showed D-MPNNs have superior performance to other tools such as gradient boosted trees for property prediction tasks \cite{Yang2019}. Formally, a molecule in a D-MPNN is considered to be a graph $G$ with edges $e_{vw}$ and nodes $v$ and $w$ with atom features $x_v$. A message passing update $m_{vw}^{t}$ is as follows:

\begin{equation}
    m_{vw}^{t+1} = \sum_{w\in N(v)} M_t(h_v^t, h_w^t, e_{vw})
\end{equation}

\begin{equation}
    h_v^{t+1} = U_t(h_v^t, m_v^{t+1})
\end{equation}

where $M_t$ is the message function, $U_t$ is the update function and $h_v^{t}$ is the hidden state at step $t$. To obtain predictions $\hat y$, the outputs of the last message passing step $T$ are passed through a feed forward network $R$ in a readout phase:
 
\begin{equation}
    \hat y = R(h_v^T \in G)
\end{equation}

In addition to using outputs of the message passing steps as input to the feed forward network, additional features $f$ can also be added:

\begin{equation}
    \hat y = R(h_v^T \in G, f)
\end{equation}

In our case of predicting activity coefficients of binary mixtures at atmospheric pressure, we treat the temperature and composition as additional features. Therefore, the feed forward network can be written as:

\begin{equation}
   \ln \gamma(x,T)= R(h_v^T \in G, x, T)
\end{equation}

Note that we predict the natural logarithm of the activity coefficient since these values can vary over an order of magnitude.

% \noindent
% \textbf{DeepGamma Polynomial:} Often, adding chemical knowledge into machine learning models can make them more accurate. In the case of activity coefficient prediction, it would be best to fit a thermodynamically consistent model such as the the non-random two liquid model (NRTL) \cite{Renon1968}. However, we found that many of the mixtures in our training set (see "Datasets") were difficult to fit using NRTL, often because the large correlations between its parameters \cite{Holler2019}. As shown in Figure \ref{fig:mnae_distribution}, the NRTL model had significant errors (>0.2) for many mixtures.

% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{gfx/Chapter07/error_distribution_fitting_cosmo.png}
%     \caption{Error distribution of mean normalized error of data fit using NRTL and polynomial models.}
%     \label{fig:mnae_distribution}
% \end{figure}

% I found empirically that the activity coefficient curves fit well to a fourth order polynomial model. While this  polynomial form does not necessarily ensure Gibbs-Duheim thermodynamic consistency\footnote{The Gibbs-Duheim equation relates changes in chemical potential to changes in temperature and pressure. $\sum_{i=1}^I N_i d\mu_i$ = -SdT + VdP. At equilibrium, the right hand side becomes zero, so traditional thermodynamic models like NRTL obey the equation $\sum_{i=1}^I N_i d\mu_i=0$.}, it should work well for the sole purpose of vapor liquid equilibrium activity coefficients (not liquid-liquid equilibrium for example). Therefore, I fitted fourth order polynomials to the activity coefficient data and used the D-MPNN to predict the coefficients of the polynomial model:

% \begin{equation}
%     \ln \gamma_i(x_i,T) = \sum_{j=0}^4 c_{ij}(T)x^j
% \end{equation}

% \begin{equation}
%      \mathbf{c_{ij}} = R(h_v^T \in G, \mathbf{x}, T)
% \end{equation}

% In the results, I compare direct prediction of activity coefficients and predicting polynomial coefficients.

\subsection{Datasets}

Previous work has demonstrated the power of transfer learning for improving predictions of D-MPNNs \cite{Vermeire2021}. I aim to obtain similar results for activity coefficient prediction, relying on two datasets.

\noindent
\textbf{Combisolv Solvation Energy Dataset:} Solvation energies describe the change in free energy when a gas molecule of a solute is placed into a solvent. Solvation energies are closely related to activity coefficients at infinite dilution \cite{Moine2017}, so encoder representations learned on this task could be useful for the downstream task of activity coefficient prediction. I utilize the Combisolv dataset, which contains the largest number of solvation energies publicly available to date: one million binary pairs of molecules calculated using COSMO-RS \cite{Vermeire2021}.

\noindent
\textbf{COSMO-RS Activity Coefficient Dataset:} I executed COSMO-RS calculations for over 18 million activity coefficients by taking all the binary pairs of 460 common solvent molecules from a previously published dataset \cite{Amar2019}. The activity coefficients were calculated in 5 K temperature increments between the normal boiling points of the each binary pair, and 0.1 mol/mol composition grid was used. All activity coefficients were calculated at atmospheric pressure. COSMOtherm 2020 at the TZVPD fine fidelity level was utilized, and parameters were taken the 2020 COSMObase database \cite{Klamt2010}. These calculations took 41 days on a 24 Intel Xeon(R) E5-2643 v3 CPUs. 

% \noindent
% \textbf{Aspen Activity Coefficient Dataset:} The third set of data were obtained using the process modeling software, Aspen Plus. This was chosen due to the fact that the thermodynamic data used to simulate processes were obtained from the Dortmund Data Bank (DDB). A total of 2500 components were added to Aspen, from which temperature dependent non-random two liquid model binary parameters of binary VLE systems were retrieved via regression of literature data from Aspen’s VLE-IG databank. VLE systems were retrieved, from which the activity coefficients were calculated at mole fractions of zero to one in increments of 0.1 and 11 K temperature increments. 


\section{Results and Discussion}

\begin{table}
    \centering
    \caption{Validation and test mean absolute error of DeepGamma (DG) models on the COSMO-RS Activity  Coefficient Dataset. Best results are bolded. TLCB stands for transfer learning, where the encoder from the model trained on the Combisolv dataset is frozen and only the feedforward network is tuned. DGP stands for models which predict polynomial coefficients instead of activity coefficients directly. Time is training time in hours.}
    % \renewcommand{\arraystretch}{1.2}
    \label{cosmo_rs_results}
    \begin{tabular}{llrrrr}
    \toprule
    Holdout Set & Model Name &  $ln\gamma_1$ MAE &   $ln\gamma_2$ MAE \\
    \midrule
    Test INDP & DG & \textbf{0.08} &\textbf{ 0.09} \\
              & DG-TLCB & \textbf{0.12} & \textbf{0.11} \\
    Test MIX & DG & \textbf{0.1} & \textbf{0.11} \\
              & DG-TLCB & 0.15 & 0.17 \\
    Valid CONT & DG & \textbf{0.05} & \textbf{0.06} \\
              & DG-TLCB & 0.06 & 0.06 \\
    Valid INDP & DG & \textbf{0.09} &\textbf{ 0.10} \\
              & DG-TLCB & 0.11 & 0.11 \\
    Valid MIX & DG & \textbf{0.08} & \textbf{0.090} \\
              & DG-TLCB & 0.10 & 0.10 \\
    \bottomrule
    \end{tabular}
\end{table}


\textit{DeepGamma} is rapidly trained to reproduce results of COSMO-RS for activity coefficient prediction. It takes 1 day to train our best performing model, and predictions on more than 1M activity coefficients takes less than 30 minutes on a modern GPU. This is significant because generating the original data required over one month, representing a 1900x speed up.

% In all our tables, we report mean absolute error for each model and holdout set. We also report the time required for training, as this varies significantly between the full and pretrained models. Fitting the polynomial model required approximately 24 hours.

Results of training on the COSMO-RS activity coefficient dataset are shown in Table \ref{cosmo_rs_results}, the base DeepGamma model achieves the lowest mean absolute error on all holdout sets. Similar mean absolute errors are achieved on the validation and test datasets for all models. As expected, the Valid CONT holdout set has the lowest MAE due to the same molecules being in the training set and holdout set.  For the best performing base model, the INDP and MIX datasets have similar error profiles, indication that model has learned a good encoder representation.  Transfer learning models have slightly worse performance than the base models, but the transfer learning models take ~50 \% less time to train. 
% Interestingly, the DeepGamma Polynomial models have the worst performance of all models. Furthermore, there are significant differences in error between the different activity coefficients predicted by the DeepGamma Polynomial.  It is possible that further hyperparameter tuning could improve the accuracy of the model, particularly through changes in the encoder and feedforward network hidden size \cite{Yang2019, Vermeire2021}.


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\textwidth]{gfx/Chapter07/cosmo_base_test_indp_parity_plot.png}
%     \caption{Parity plot showing predicted versus ground truth activity coefficients obtained from COSMO-RS.}
%     \label{fig:my_label}
% \end{figure}


\section{Conclusions}

In this chapter, I propose \textit{DeepGamma}, an approach to predicting activity coefficients directly from molecular structures using directed messasge passing neural networks. Furthermore, the model is accurate at predicting activity coefficients of unseen molecules. However, I found that transfer learning did not improve the accuracy of the model but did reduce training time. This lack of improved performance is likely due to the discrepancy between the pretraining task (Gibbs free energy of solvation) and the downstream task of activity coefficient prediction. In particular, the pretraining task only used data a 298K, while my dataset used variable temperature. Recent work \cite{Lansford2023} has shown that learning temperature dependent data can lead to different topology in the trained neural network and thus makes it difficult to transfer from a model trained at a single temperature to one at multiple temperatures.

\textit{DeepGamma} has several limitations. First,  it relies solely on quantum simulation data, which are themselves not fully predictive of experimental results. This could be improved in the future by training on experimental data as is done in the next chapter. Second, the results are not Gibbs-Duheim  consistent. Since \textit{DeepGamma} was published, several works have proposed to overcome this challenge by incorporating more physical constraints into the neural network. For example, Sanchez Medina used  a Gibbs-Helmholtz-derived expression as the last layer of their readout feed forward network to ensure thermodynamic consistency \cite{SanchezMedina2023}. In a similar approach Winter et al developed a model for predicting activity coefficients using transformer models. Specifically, they predict the parameters of the NRTL model, for any mixture \cite{Winter2022}. An alternative strategy was proposed in a paper I recently coauthored with Jan Rittig. We used the Gibbs Duheim equation (shown below) as a constraint in the loss function and showed that doing so improved both consistency and accuracy of the predicted activity coefficients \cite{Rittig2023b}.

\begin{equation}
    x_1 \biggl (\frac{\partial \ln \gamma_1}{\partial x_1}\biggr)_{T,p} + x_2 \biggl (\frac{\partial \ln \gamma_2}{\partial x_2}\biggr)_{T,p} = 0
\end{equation}

In the next chapter, I propose a different approach to thermodynamic predictions that leverages equations of state. This enables prediction of multiple properties that are thermodynamically consistent.

% *****************************************
% *****************************************
% *****************************************
% *****************************************
% *****************************************
