%************************************************
\chapter{DeepGamma: A deep learning model for activity coefficient prediction}\label{ch:deep_gamma} 
%************************************************
This chapter is based on the following paper:
\begin{enumerate}
\item \textbf{Felton K.}, Ben-Safar, H., and Lapkin A. \textit{AAAI Workshop on AI to Accelerate Science and Engineering}. "\textit{DeepGamma}: A deep learning model for activity coefficient prediction." 2022.
\end{enumerate}
\textbf{My contribution}: I executed the COSMO-RS calculations, trained the final version of the \textit{DeepGamma} models and wrote the manuscript.

\section{Introduction}

Vapor-liquid equilibrium thermodynamics play an essential role in a wide variety of fields in the basic and applied sciences. For example, chemical reactions often happen in between the vapor and liquid phase, where vapor produced by a reaction is extracted to drive conversion or a reactant is introduced as a vapor. Similarly, large scale chemical processes require separation of mixtures into components to purify a valuable product. One ubiquitous method for achieving such separations is distillation, which relies on differences in boiling points of mixtures to achieve separation.

In order to design and engineer systems which contain vapor-liquid equilibrium, thermodynamic equations are utilized. Thermodynamic equations describe the relationship between the composition of the liquid and the vapor at a given temperature and pressure. One well-known thermodynamic equation is Raoult's Law: 

\begin{equation}
    \label{ideal-gas}
    y_i P = x_iP_i^{sat}(T)
\end{equation}

where $y_i$ and $x_i$ are the vapor and liquid compositions of component $i$ of the mixture respectively, $P$ is the absolute pressure, and $P_i^{sat}(T)$ is the vapor pressure at temperature $T$. Raoult's law describes non-interacting ideal systems, yet it fails to properly predict vapor-liquid equilibrium for the wide variety of mixtures used in industrial chemical processes. Therefore, the simplified gamma-phi equation was developed to describe deviations from ideality:

\begin{equation}
    \label{gamma-phi}
    y_i P = x_i \gamma_i(x,T,P)   \exp \biggl(\frac{V_i^L(P-P_i^{sat}(T))}{RT}\biggr)
\end{equation}

where $\gamma_i(\mathbf x,T,P)$ is the activity coefficient at liquid composition $\mathbf x$,  temperature $T$, and pressure $P$; $V_i^L$ is the specific volume and $P_i^{sat}$ is the vapor pressure.  

Activity coefficients are the unknown parameters in the gamma-phi thermodynamic equation that correct for deviations from ideality. These activity coefficients must be predicted for each new mixture using thermodynamic models such as the non-random two liquid model \cite{Renon1968}. 

Unfortunately, the vapor-liquid equilibrium experiments required to parameterize thermodynamic models for predicting activity coefficients are notoriously time-intensive to complete. Classical thermodynamic measurement apparatus such as stills require equilibrium to be reached at each temperature and pressure combination before a measurement is taken \cite{Ronc1976, Dechambre2014}. This equilibration process can take anywhere from minutes to hours, meaning a full set of experiments for a new mixture can take weeks to months. As a result, vapor liquid equilibrium experiments are executed very selectively.

To reduce the time required for parameterizing thermodynamic models for predicting activity coefficients, there is a large body of research focused on directly predicting the parameters from molecular structures. This work ranges from simple counting of functional groups \cite{Fredenslund1975} to density functional theory \cite{ Klamt2010} and machine learning \cite{Urata2002, Nami2011,  Jirasek2020}.  However, these prediction approaches often only cover a small subset of all mixture types (e.g., only hydrocarbons) or require significant calculation times. Thus, there is a need for a general, fast and accurate method for predicting activity coefficients \textit{a priori}.

In this chapter, I introduce \textit{DeepGamma}, a deep learning model for fast predictions of activity coefficients directly from molecular structures.  I leverage message passing neural networks (MPNN) to reproduce the results of quantum simulations for predicting activity coefficients without computational expense. Specifically, this model decreases the time required for calculations by a factor of 1900. In the following section, I briefly review methods for predictive thermodynamics before discussing the methodology and results.

\section{Background}

I classify existing methods for predictive thermodynamics into three categories: group-contribution methods, quantum chemistry based methods and machine learning methods. 

\subsubsection{Group Contribution Methods}

Group contribution methods such as UNIFAC were originally developed in the 1970s and predict the activity coefficient as a weighted sum of the occurrence of a predetermined set of basis functions with inputs as the chemical functional groups in each component \cite{Fredenslund1975}. UNIFAC specifically predicts the natural logarithm of the activity coefficient of component $i$ in  a mixture as the sum of a combinatorial term $\ln \gamma_i^C$ and a residual term $\ln \gamma_i^R$:

\begin{equation}
    \ln \gamma_i = \ln \gamma_i^C + \ln \gamma_i^R
\end{equation}

The combinatorial term represents the combination of different functional groups:

\begin{equation}
    \ln  \gamma^C_i = \ln \frac{\Phi_i}{x_i} + \frac{z}{2} q_i \ln \frac{\theta_i}{\Phi_i} + l_i - \frac{\Phi_i}{x_i} \sum_jx_jl_j
\end{equation}

where $\Phi_i$ and $\theta_i$ are the weighted molar segment area and area for component. These variables in addition to $l_j$ are a function of $r_i$, $z$ and $q_i$, which are  in turn functions of the group volume and surface area parameters $R_k$ and $Q_k$ for tabulated functional groups:

\begin{equation}
    \Phi_i  = \frac{r_i x_i}{\sum_j r_j x_j}
\end{equation}

\begin{equation}
    \theta_i = \frac{q_i x_i}{\sum_j q_j x_j}
\end{equation}

\begin{equation}
    l_i = \frac{z}{2}(r_i-q_i) - (r_i -1) \;\; z=10
\end{equation}

\begin{equation}
    r_i  = \sum_k = v_k^i R_k
\end{equation}

\begin{equation}
    q_i = \sum_k v_k^i Q_k
\end{equation}

$v_k^i$ is the number of groups of type $k$ in component $i$.

The residual term represents inter-group interactions. It is composed of the group residual activity coefficient $\Gamma_k$ and  $\Gamma_k^{(i)}$, the residual activity coefficient of group k in reference solution with only component $i$.

\begin{equation}
    \ln \gamma^R_i = \sum_k v_k^{(i)}(\ln \Gamma_k - \ln \Gamma_k^{(i)})
\end{equation}

\begin{equation}
    \ln \Gamma_k = Q_k \biggl[1 - \ln \bigl(\sum_m \theta_m \psi_{mk}\bigr) - \sum_m \frac{\theta_m \Psi_{km}}{\sum_n \theta_n \Psi_{nm}} \biggr]
\end{equation}

\begin{equation}
    \theta_m = \frac{Q_m X_m}{\sum_n Q_n X_n}
\end{equation}

\begin{equation}
    \Psi_{mn} = \exp{\frac{a_{mn}}{T}}
\end{equation}

where $a_{mn}$ is a binary interaction parameter. 

While UNIFAC provides fast predictions, the accuracy of the model is limited by the interactions explicitly accounted for in each basis function. It can be challenging to hand-craft all such interactions as they can often extend between several atoms.

\subsubsection{Quantum Chemistry}

The second set of methods are quantum chemistry methods. These simulations often use a combination of molecular dynamics and density functional theory (DFT) calculations to predict activity coefficients for each mixture component \cite{Constantinescu2005}. COSMO-RS is one of the most reliable computational methods for liquid-phase thermodynamic predictions \cite{Klamt1995, Klamt2010}. It relies on the theory of screening charges, which states that every element of a surface of a dissolved solute must be complemented by an opposite charge in the solvent  \cite{Klamt1995}. The charge surface around a solute is divided into infinitesimally small pieces, which are then integrated to find the charge density. This charge density can then be used to calculate activity coefficients. The charge density is calculated via rigorous DFT calculations. COSMO-RS and a related method named COSMO-SAC have been applied in calculations of VLE \cite{Constantinescu2005}, liquid-liquid equilibrium (LLE) \cite{Dechambre2014} and vapor-liquid-liquid-equilibrium (VLLE) curves \cite{Kundu2011}.

The downside of COSMO methods is that they are often not accurate for polar compounds \cite{Constantinescu2005, Kundu2011}. This is often due to the lack of theory for the hydrogen bonding present in these systems \cite{Kundu2011}. Another challenging aspect of COSMO-RS is the computational intensity of DFT calculations for new mixtures. 

\subsubsection{Machine Learning}

There has been significant work on directly predicting infinite dilution activity coefficients from molecular structures and closely related free solvation energies using simple neural networks \cite{Urata2002, RamirezBeltran2009, Nami2011, Behrooz2017}, matrix completion\cite{Jirasek2020} and  graph neural networks \cite{Vermeire2021, Felton2022, SanchezMedina2022, Qin2022, Rittig2022}. However, the limitation of these works is their lack of thermodynamic consistency and other guarantees that come with rigorously derived equations of state. To overcome this problem, recent work has attempted to predict EoS parameters \cite{Abbasi2020, Madani2021, Abdallahelhadj2022, Winter2022}, which can be in turn used to predict thermodynamic outcomes.
 
\section{Methods}
\subsection{Models}

\noindent
\textbf{Message passing neural networks:} Molecules can be treated as graphs with atoms as nodes and bonds as edges. Therefore, message passing neural networks (MPNN) that operate on graphs can be used for end-to-end prediction of molecular properties \cite{Gilmer2017}.  In a MPNN, each node passes a vector (i.e., its current features) to its neighbor. This  vector is called a message. In each layer, a permutation invariant aggregation function such as the mean is used to calculate the total message for each node prior to passing through an activation function to get a new value for each node.  In contrast to traditional fixed fingerprints like ECFP \cite{Rogers2010}, the best feature vector is learned end-to-end for each property prediction task.

One type on MPNN is a directed message passing neural network (D-MPNN) in which the encoder acts on edges (bonds) instead of nodes (atoms) to improve stability of training \cite{Yang2019}. Yang, Swanson and colleagues demonstrated that D-MPNNs have superior performance to other tools such as gradient boosted trees for property prediction tasks \cite{Yang2019}. Formally, a molecule in a D-MPNN is considered to be a graph $G$ with edges $e_{vw}$ and nodes $v$ and $w$ with atom features $x_v$. A message passing update $m_{vw}^{t}$ is as follows:

\begin{equation}
    m_{vw}^{t+1} = \sum_{w\in N(v)} M_t(h_v^t, h_w^t, e_{vw})
\end{equation}

\begin{equation}
    h_v^{t+1} = U_t(h_v^t, m_v^{t+1})
\end{equation}

where $M_t$ is the message function, $U_t$ is the update function and $h_v^{t}$ is the hidden state at step $t$. To obtain predictions $\hat y$, all of the atom vectors are added or averaged in a step often called pooling. Since activity coefficients are an intensive property (i.e., they do not depend directly on molecular size), I use the mean pooling function \cite{Schweidtmann2023}:

\begin{equation}
    h_v^{'T} = \frac{1}{N(v)} \sum_{w\in N(v)} h_v
\end{equation}

the outputs of the last message passing step $T$ are passed through a feed forward network $R$ in a readout phase:
 
\begin{equation}
    \hat y = R(h_v^{'T} \in G)
\end{equation}

In addition to using outputs of the message passing steps as input to the feed forward network, additional features $f$ can also be added:

\begin{equation}
    \hat y = R(h_v^T \in G, f)
\end{equation}

In our case of predicting activity coefficients of binary mixtures at atmospheric pressure, we treat the temperature and composition as additional features. Therefore, the feed forward network can be written as:

\begin{equation}
   \ln \gamma(x,T)= R(h_v^T \in G, x, T)
\end{equation}

Note that we predict the natural logarithm of the activity coefficient since these values can vary over an order of magnitude.

% \noindent
% \textbf{DeepGamma Polynomial:} Often, adding chemical knowledge into machine learning models can make them more accurate. In the case of activity coefficient prediction, it would be best to fit a thermodynamically consistent model such as the the non-random two liquid model (NRTL) \cite{Renon1968}. However, we found that many of the mixtures in our training set (see "Datasets") were difficult to fit using NRTL, often because the large correlations between its parameters \cite{Holler2019}. As shown in Figure \ref{fig:mnae_distribution}, the NRTL model had significant errors (>0.2) for many mixtures.

% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{gfx/Chapter07/error_distribution_fitting_cosmo.png}
%     \caption{Error distribution of mean normalized error of data fit using NRTL and polynomial models.}
%     \label{fig:mnae_distribution}
% \end{figure}

% I found empirically that the activity coefficient curves fit well to a fourth order polynomial model. While this  polynomial form does not necessarily ensure Gibbs-Duheim thermodynamic consistency\footnote{The Gibbs-Duheim equation relates changes in chemical potential to changes in temperature and pressure. $\sum_{i=1}^I N_i d\mu_i$ = -SdT + VdP. At equilibrium, the right hand side becomes zero, so traditional thermodynamic models like NRTL obey the equation $\sum_{i=1}^I N_i d\mu_i=0$.}, it should work well for the sole purpose of vapor liquid equilibrium activity coefficients (not liquid-liquid equilibrium for example). Therefore, I fitted fourth order polynomials to the activity coefficient data and used the D-MPNN to predict the coefficients of the polynomial model:

% \begin{equation}
%     \ln \gamma_i(x_i,T) = \sum_{j=0}^4 c_{ij}(T)x^j
% \end{equation}

% \begin{equation}
%      \mathbf{c_{ij}} = R(h_v^T \in G, \mathbf{x}, T)
% \end{equation}

% In the results, I compare direct prediction of activity coefficients and predicting polynomial coefficients.

\subsection{Datasets}

Previous work has demonstrated the power of transfer learning for improving predictions of D-MPNNs \cite{Vermeire2021}. I aim to obtain similar results for activity coefficient prediction, relying on two datasets.

\noindent
\textbf{Combisolv Solvation Energy Dataset:} Solvation energies describe the change in free energy when a gas molecule of a solute is placed into a solvent. Solvation energies are closely related to activity coefficients at infinite dilution \cite{Moine2017}, so encoder representations learned on this task could be useful for the downstream task of activity coefficient prediction. I utilize the Combisolv dataset, which contains the largest number of solvation energies publicly available to date: one million binary pairs of molecules calculated using COSMO-RS \cite{Vermeire2021}.

\noindent
\textbf{COSMO-RS Activity Coefficient Dataset:} I executed COSMO-RS calculations for over 18 million activity coefficients by taking all the binary pairs of 460 common solvent molecules from a previously published dataset \cite{Amar2019}. The activity coefficients were calculated in 5 K temperature increments between the normal boiling points of the each binary pair, and 0.1 mol/mol composition grid was used. All activity coefficients were calculated at atmospheric pressure. COSMOtherm 2020 at the TZVPD fine fidelity level was utilized, and parameters were taken from the 2020 COSMObase database \cite{Klamt2010}. These calculations took 41 days on a 24 Intel Xeon(R) E5-2643 v3 CPUs. 

% \noindent
% \textbf{Aspen Activity Coefficient Dataset:} The third set of data were obtained using the process modeling software, Aspen Plus. This was chosen due to the fact that the thermodynamic data used to simulate processes were obtained from the Dortmund Data Bank (DDB). A total of 2500 components were added to Aspen, from which temperature dependent non-random two liquid model binary parameters of binary VLE systems were retrieved via regression of literature data from Aspen’s VLE-IG databank. VLE systems were retrieved, from which the activity coefficients were calculated at mole fractions of zero to one in increments of 0.1 and 11 K temperature increments. 

\subsection{Training}
\noindent
\textbf{Holdout data:} One of the important aspects of evaluating the applicability of machine learning models for property prediction is holding out data from training to evaluate the model fairly. Random splits can artificially inflate model accuracy scores because similar or identical molecules can be placed in the train and holdout sets \cite{Kovacs2021}. Therefore I used a clustering procedure where  ECFP fingerprints with 2048 bits were generated using RDKit, and the k-means clustering algorithm \cite{MacQueen1967} was run on five dimensional projections of these fingerprints from the dimensionality reduction tool UMAP \cite{McInnes2018}. Since the data was for binary mixtures, I create three types of holdout sets to represent common use cases:

\begin{itemize}
    \item \textbf{MIX}: One of the molecules in a mixture is in the train and the other is in the holdout set.
    \item \textbf{INDP}: Both of the molecules in a mixture are not in the train set (i.e., only in the holdout set).
    \item \textbf{CONT}: A random split on molecules in the train clusters. This represents when there are some measurements of activity coefficients of both of the molecules in a mixture in the training set but not at the temperature or composition being queried in the holdout set. 
\end{itemize}

The training set size was 11M activity coefficients. Sizes of the holdout set are listed in Table \ref{tab:cosmo_rs_results}.

\noindent
\textbf{Training Details:} I leveraged the implementation of D-MPNNs in the python package chemprop \cite{Yang2019, Heid2023}. A model was trained on the Combisolv dataset using the same hyperparameters as in the paper by Vermeire and Green (batch size 50, encoder hidden size 200, and feed forward network hidden size of 500) except that a shared encoder was used instead of one for the solute and solvent \cite{Vermeire2021}. Using a shared encoder did not have a significant impact on the results. Since the paper did not report learning rate, a small set of experiments were conducted to find that an initial learning rate of 1e-4 and max learning rate of 2e-4 were optimal for the Noam learning rate scheduler \cite{Vaswani2017}. Training on a single Tesla GPU (AWS EC2 g4dn.xlarge) required 43 hours. For the COSMO-RS models, we used a feedforward network hidden size of 380, a batch size of 4550, and a max learning rate of 6e-3.  The COSMO-RS models were trained for until three consecutive epochs did not have improved validation MSE. I used instances equipped with a single A10 GPU (16 GB) and 61 GB of RAM to train these models. For transfer learning on the COSMO-RS dataset, I froze the encoder learned for the Combisolv model and only adjusted the parameters of the feed forward network.


\section{Results and Discussion}

\begin{table}
    \centering
    \caption{Validation and test mean absolute error of DeepGamma (DG) models on the COSMO-RS Activity  Coefficient Dataset. TLCB stands for transfer learning, where the encoder from the model trained on the Combisolv dataset is frozen and only the feedforward network is tuned. $n$ is the size of the holdout set.}
    % \renewcommand{\arraystretch}{1.2}
    \label{tab:cosmo_rs_results}
    \begin{tabular}{llrrrr}
    \toprule
    Holdout Set & $n$ & Model Name &  $ln\gamma_1$ MAE &   $ln\gamma_2$ MAE \\
    \midrule
    Test INDP & DG & 0.1453 & 0.1369 \\
              & DG-TLCB & 0.1345 & 0.1355 \\
    Test MIX & DG & 0.1077 & 0.1033 \\
              & DG-TLCB & 0.0955 & 0.0979 \\
    Valid CONT & DG & 0.0270 & 0.0267 \\
              & DG-TLCB & 0.0291 & 0.0323 \\
    Valid INDP & DG & 0.0747 & 0.0717 \\
              & DG-TLCB & 0.1076 & 0.0930 \\
    Valid MIX & DG & 0.0623 & 0.0575 \\
              & DG-TLCB & 0.0826 & 0.0793 \\
    \bottomrule
    \end{tabular}
\end{table}


\textit{DeepGamma} is rapidly trained to reproduce results of COSMO-RS for activity coefficient prediction. It takes 1 day to train our best performing model, and predictions on more than 1M activity coefficients takes less than 30 minutes on a modern GPU. This is significant because generating the original data required over one month, representing a 1900x speed up.

% In all our tables, we report mean absolute error for each model and holdout set. We also report the time required for training, as this varies significantly between the full and pretrained models. Fitting the polynomial model required approximately 24 hours.

Results of training on the COSMO-RS activity coefficient dataset are shown in Table \ref{tab:cosmo_rs_results}. The base \textit{DeepGamma} model  and model pretrained on COSMO-RS data achieve similar mean absolute errors. Similar mean absolute errors are achieved on the validation and test datasets for all models. As expected, the Valid CONT holdout set has the lowest MAE due to the same molecules being in the training set and holdout set.  For the best performing base model, the INDP and MIX datasets have similar error profiles, indication that model has learned a good encoder representation.  Note that even though the transfer learning model has approximately the performance as the base models, it takes ~50 \% less time to train. 

The accuracy of predictions varies as function of composition. As shown in Figure \ref{fig:absolute_error_composition}, there are higher errors approaching infinite dilution, which is a trend seen in other work published contemporaneously \cite{Winter2022}. This is likely due to the activity coefficient being larger approaching infinite dilution.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{gfx/Chapter07/DG_test_indp_absolute_error_vs_composition.png}
    \caption{Absolute error as a function of composition for the Test INDP predictions from the DG model.}
    \label{fig:absolute_error_composition}
\end{figure}

Figure \ref{fig:dg_test_indp_parity_plot} visualizes the predictions versus original COSMO-RS data for the test INDP set. The predictions are most accurate for activity coefficients between 0 and 5 but become less accurate when the ground truth has a large absolute value. This is primarily due to there being significantly more data in the range of 0 to 5 than the extremes. There also seems to be a variation in the errors between the data for the first and second activity coefficient, whereas one would expect a thermodynamic model to have the same error distribution for both activity coefficients since they are interchangeable. This discrepancy in consistency points to the fact that \textit{DeepGamma} lacks the thermodynamic consistency guarantees provided by classical thermodynamic models.
 
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{gfx/Chapter07/DG_test_indp_parity_plot.png}
    \caption{Parity plot showing predicted versus ground truth activity coefficients obtained from COSMO-RS.}
    \label{fig:dg_test_indp_parity_plot}
\end{figure}

\subsection{Using \textit{DeepGamma} for VLE predictions}

I used \textit{DeepGamma} to predict the activity coefficients of several mixtures in the test INDP set and used these activity coefficients to construct P-xy diagrams. I used Antoine equation with activity coefficients from the NIST database to calculate vapor pressures \cite{Muzny}. In Figure \ref{fig:low_error_dg_prediction} and \ref{fig:high_error_dg_prediction}, I show two mixtures with different levels of error  between the \textit{DeepGamma} and COSMO-RS predictions. Figure \ref{fig:low_error_dg_prediction} shows 2-methyl-2-butanol and 2-methylpentane at 333.15K, a mixture for which \textit{DeepGamma} accurately predicts activity coefficients and thus renders an accurate representation of the P-xy diagram.  However, in Figure \ref{fig:high_error_dg_prediction}, I show isopropylamine and trifluoroacetic acid, a mixture for which \textit{DeepGamma} has high erorr in the predictions liquid pressure and even gives non-physical results from vapor pressure. This again highlights that the lack of thermodynamic consistency in deep learning models represents a problem when using the predictions from models like \textit{DeepGamma} in downstream applications.

\begin{figure}
    \centering
    \includegraphics[trim=0 0 0 40, clip, width=0.8\textwidth]{gfx/Chapter07/low_error_pxy.png}
    \caption{P-xy diagram for 2-methyl-2-butanol and 2-methylpentane at 333.15K. Predictions from \textit{DeepGamma} are green circles, while the black lines are COSMO-RS. Both molecules were not in the training set (i.e., part of the test INDP set).}
    \label{fig:low_error_dg_prediction}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[trim=0 0 0 40, clip, width=\textwidth]{gfx/Chapter07/high_error_pxy.png}
    \caption{P-xy diagram for isopropylamine and trifluoroacetic acid at temperatures between 305.15 K and 330.15 K. Predictions from \textit{DeepGamma} are green circles, while the black lines are COSMO-RS. Both molecules were not in the training set (i.e., part of the test INDP set).}
    \label{fig:high_error_dg_prediction}
\end{figure}



\section{Conclusions}

In this chapter, I propose \textit{DeepGamma}, an approach to predicting activity coefficients directly from molecular structures using directed messasge passing neural networks. Furthermore, the model is accurate at predicting activity coefficients of unseen molecules. However, I found that transfer learning did not improve the accuracy of the model but did reduce training time. This lack of improved performance is likely due to the discrepancy between the pretraining task (Gibbs free energy of solvation) and the downstream task of activity coefficient prediction. In particular, the pretraining task only used data a 298K, while my dataset used variable temperature. Recent work \cite{Lansford2023} has shown that learning temperature dependent data can lead to different topology in the trained neural network and thus makes it difficult to transfer from a model trained at a single temperature to one at multiple temperatures.

\textit{DeepGamma} has several limitations. First,  it relies solely on quantum simulation data, which are themselves not fully predictive of experimental results. This could be improved in the future by training on experimental data as is done in the next chapter. Second, the results are not Gibbs-Duheim  consistent. Since \textit{DeepGamma} was published, several works have proposed to overcome this challenge by incorporating more physical constraints into the neural network. For example, Sanchez Medina used  a Gibbs-Helmholtz-derived expression as the last layer of their readout feed forward network to ensure thermodynamic consistency \cite{SanchezMedina2023}. In a similar approach Winter et al developed a model for predicting activity coefficients using transformer models. Specifically, they predict the parameters of the NRTL model, for any mixture \cite{Winter2022}. An alternative strategy was proposed in a paper I recently coauthored with Jan Rittig. We used the Gibbs Duheim equation (shown below) as a constraint in the loss function and showed that doing so improved both consistency and accuracy of the predicted activity coefficients \cite{Rittig2023b}.

\begin{equation}
    x_1 \biggl (\frac{\partial \ln \gamma_1}{\partial x_1}\biggr)_{T,p} + x_2 \biggl (\frac{\partial \ln \gamma_2}{\partial x_2}\biggr)_{T,p} = 0
\end{equation}

In the next chapter, I propose a different approach to thermodynamic predictions that leverages equations of state. This enables prediction of multiple properties that are thermodynamically consistent.

% *****************************************
% *****************************************
% *****************************************
% *****************************************
% *****************************************
