%************************************************
\chapter{Introduction}\label{ch:introduction}
%************************************************

Fine chemicals manufacturers are under pressure to reduce the cost and environmental impact of their processes. These businesses produce high value chemical products such as pharmaceuticals and agrochemicals, where a regulatory body needs to approve each product and the process used to manufacture it. This approval process can take years and result in millions of dollars in development costs \cite{Prasad2017}. Consequently, reducing cost during product development is a primary target for many fine chemicals manufacturers. Ideally, this would be achieved by "Right First Time" scale-up, where processes can be scaled from the laboratory to manufacturing with minimal effort \cite{Poechlauer2013}. In addition to cost concerns, environmental concerns come from the fact that fine chemicals processes produce the most industrial waste and use the most energy per unit of product among all chemical processes (e.g., commodity, speciality) \cite{Sheldon2018}. Therefore, many fine chemicals companies have committed to making significant reductions in their waste production and energy usage over the next ten years to meet goals set by governing bodies \cite{BASF2020}.

% One of the barriers to reducing the cost and environmental impact of fine chemicals development is batch processing (BP).  In BP, each chemical reaction is carried out independently before moving to the next step. BP is used in early stage research to discover chemical routes, and, as shown in Figure \ref{batch_v_flow}, this batch approach is scaled to the pilot plant, where 10-500 kg of new products are manufactured for regulatory approval. Batch processes are notoriously difficult to scale up from the laboratory to the pilot plant. For example, it is not uncommon for a reaction that works perfectly at the laboratory scale to fail in the pilot plant due to insufficient heat and mass transfer in a larger reactor. These "scale-up failures" are costly because the reactants and reagents used in fine chemicals reactions are often expensive. Additionally, multiple batches are usually required in BP, and large volumes of solvent are used to clean the reactors between batches \cite{Lee2016, Sheldon2018}. This explains, in part, why solvents are the main contributor to the environmental waste from fine chemicals processes \cite{Rogers2019}.

% \begin{figure}[bt]
%   \includegraphics[height=0.2\textheight]{gfx/Chapter01/scale_up_schematic.png}
%   \caption{Schematic comparing batch and flow chemistry. Batch processing (BP) achieves scale by increasing volume, while continuous processing (CP) achieves scale by continuous production over time.}
%   \label{batch_v_flow}
% \end{figure}

% Continuous processing (CP) offers a potentially low cost and environmentally friendly method for uninterrupted manufacturing. Material flows continuously between units, so the process can be run for several days without interruption to produce large volumes of material. CP removes the need for cleaning equipment during a campaign, which reduces solvent use significantly \cite{Lee2016}. Furthermore, pilot plant CP equipment is often an order of magnitude smaller than traditional batch processing due to its higher space time yield \cite{Elvira2013}. However, realizing these benefits requires optimized processes. For example, when optimizing chemical reactions for CP, process engineers need to find reaction conditions that balance maximizing throughput and minimizing impurity generation and solvent use. Previous analysis has suggested that many reactions have over 50 million potential conditions \cite{Murray2013}, so simple brute-force screening is not sufficient since experiments are expensive and time-consuming.

Therefore, there is a strong need to develop approaches that can quickly optimize continuous processes when only limited experimental data is available. Recently, machine learning has been proposed as a solution to reducing the experimental burden for process development \cite{Taylor2023a}.  However, machine learning methods either rely on a large amount of existing data or require gathering new data, which is expensive and time consuming.  One approach to alleviate the data limitations is transfer learning. Transfer learning leverages knowledge gained from one task and applies it to another \cite{Zhuang2021}, an in other domains, transfer learning has shown promise for building machine learning that works with small amounts of data. For example, most image classification models are now pretrained on a large dataset of general images, so training on a new classification task only requires a small number of images \cite{He2016}. Similar results have been seen in natural language processing \cite{Brown2020} and even drug discovery \cite{Ramsundar2017}. 

Considering developments in transfer learning, this thesis aims accelerate chemical process development in the pharmaceutical and speciality chemicals industries by reducing the number of experiments required to bring a new process to market. The overarching question of the thesis is as follows:
\begin{displayquote}
Can transfer learning be used to accelerate process development of separations and reactions by leveraging data from multiple sources? 
\end{displayquote}
Chapter \ref{ch:background} contains an overview of transfer learning. I review the key concepts underlying transfer learning and demonstrate how transfer learning can be used in the context of the machine learning algorithms applied in this thesis.  The subsequent chapters are split into three parts.

In Part I (chapters \ref{ch:summit} and \ref{ch:mtbo}), I focus on reaction optimisation, an essential part of achieving high efficiency continuous processes. In Chapter \ref{ch:summit}, I review the existing literature on automated reaction optimisation and introduce Summit, a framework for benchmarking machine learning for reaction optimisation.  In Chapter \ref{ch:mtbo}, I demonstrate how a transfer learning technique, namely multitask Bayesian optimisation, can accelerate reaction optimisation. I present \textit{in silico} benchmarking of multitask Bayesian optimisation using Summit and results from collaborative work with experimentalists.

In Part II (Chapters \ref{ch:rl_tuning} and \ref{ch:mfbo}), I turn to controller tuning for laboratory distillation columns. Laboratory distillation columns are utilized to separate novel mixtures in regulatory approval campaigns and to ensure that separation of a novel mixture works before investing in building a new large scale distillation column. In Chapter \ref{ch:rl_tuning}, I review the literature on controller tuning and present an initial failed attempt at using reinforcement learning to tune PID controllers. In Chapter \ref{ch:mfbo}, I present an alternative approach that relies on rigorous dynamic simulation and a transfer learning technique called multifidelity Bayesian optimisation.

In Part III (Chapters \ref{ch:deep_gamma} and \ref{ch:ml_saft}), I examine methods for predictive thermodynamics. My interest in this topic came from the challenge of simulating distillation columns for new mixtures, which requires vapor liquid equilibrium (VLE) data. In Chapter \ref{ch:deep_gamma}, I introduce \textit{DeepGamma} a method for predicting activity coefficients using deep learning. I show that, in this case, transfer learning does not improve predictions. Finally, in Chapter \ref{ch:ml_saft}, I introduce ML-SAFT, a framework for predicting the parameters of the PCP-SAFT equation of state. I show that in this case, transfer learning from simulations to experimental data can improve predictions of thermodynamic quantities, potentially saving experimental time and cost.

%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************




