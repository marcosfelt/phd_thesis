%************************************************
\chapter{Discussion}\label{ch:discussion} 
%************************************************

As the fine chemicals industry aims to become more cost efficient and environmentally friendly, optimisation of processes at the early stage will become critical. In particular, continuous processing (CP) will play a large role, but the speed of adoption has historically been slow due the challenges associated with developing robust flow chemistry processes.

\section{Thesis Contributions}

In this thesis, I aimed to address the question "Can transfer learning accelerate process development?"  Transfer learning provides a series of methods to leverage data from various sources to improve machine learning models. The work presented in this thesis provides frameworks for integrating data from various sources, the often overlooked and challenging aspect of transfer learning. With these frameworks in place, I demonstrated that transfer learning can indeed accelerate process development in several ways. While I do not focus exclusively on CP, many of the examples from reaction optimisation and distillation are based on CP examples with industrial application as noted below.

In part I, I show that transfer learning can reduce the number of experiments required for reaction optimisation. Reaction optimisation plays a significant role in early stage process development and will be increasingly important as batch processes are translated to CP. However, testing automated optimisation strategies experimentally can be expensive from both a time and financial perspective.  Therefore, I first built Summit, a framework for benchmarking reaction optimisation \textit{in silico}. Summit enabled robust studies of various strategies for reaction optimisation, and I found that Bayesian optimisation performed better than other optimisation strategies across several chemically motivated benchmarks. Subsequently, I used Summit to conduct benchmarking studies of the transfer learning technique multitask Bayesian optimisation (MTBO) \cite{Swersky2013}. By leveraging data-driven benchmarks created from literature data, I demonstrated that MTBO could leverage data from related experiments to reduce the number of experiments required to find reaction conditions that resulted in high yield. Furthermore, in collaboration with experimentalists, I demonstrated that MTBO was effective in optimizing C-H activation reaction in a flow chemistry reactor.  Overall, this section demonstrated that value of using historical experimental data to accelerate optimisation of chemical processes, particularly in early stage development of continuous processes. 

In Part II, I developed approaches to automated controller tuning with a specific focus on distillation control. Continuous distillation columns are inherently unstable unit operations and therefore require a control system for stable operation. Although advanced control methods such as model predictive control have been shown to be effective for chemical process control, simple linear feedback controllers, particularly PID controllers are still the most commonly used in the industry due to their operational simplicity. Therefore, I focused on improving PID controller tuning. I first attempted to use reinforcement learning to tune PID controllers but found that my method was both unstable and computationally inefficient. Therefore, I took a different approach that used multifidelity Bayesian optimisation to combine simulation and experiments.  I found that using the multifidelity approach combined with a classifier to filter overly aggressive tuning resulted in better tuning in less time.


In part III, I explored methods for utilizing machine learning for predictive thermodynamics. Thermodynamics underlies many chemical process simulations including distillation simulations, yet acquiring experimental  data for new compounds is slow and expensive. I presented two approaches to predictive thermodynamics.  I first introduced \textit{DeepGamma}, a deep learning model for activity coefficient prediction. I demonstrated that \textit{DeepGamma} can reproduce data from the thermodynamics program COSMO-RS with a 1900x speed-up, but I found that pretraining  did not improve the performance of the model. This is likely due to using a transfer learning task that was substantially different from the primary task. Furthermore, while \textit{DeepGamma} predictions are accurate, they are not thermodynamically consistent and would be unusable in a real process simulation. While work published after \textit{DeepGamma} addresses this issue for activity coefficient prediction \cite{Winter2022, SanchezMedina2023}, I decided to dedicate my remaining research time to using predictive thermodynamics in the context of an equation of state that provided thermodynamic guarantees by default. Therefore, I created ML-SAFT, a framework for predicting the parameters of the PCP-SAFT equation of state and showed that random forests perform best on newly created dataset. However, I further demonstrated that pretraining on data from COSMO-RS simulations can improve the performance of message passing neural networks. This demonstrates that transfer learning can improve the performance of deep learning in the low data limit.

\section{Future directions}

Given the varied success of the transfer learning methods used in this thesis, I think there are several directions for future research.



\subsection{Automatic data collection}

While this thesis is ostensibly about transfer learning, it could have equally been titled "generating datasets and simulations for transfer learning." The majority of my time was spent on data curation and simulation frameworks which subsequently enabled transfer learning.  This observation makes it clear to me that obtaining clean datasets is still a challenge in chemistry and process systems engineering. Therefore, I would follow up this work by asking the question, "How can data for transfer learning be collected automatically?"

I would approach this question of collecting data from two directions. The first would be exploring methods to automatically extract data from existing sources. In this work, significant time was spent manually extracting and cleaning data from the academic literature, for example, to create data-driven benchmarks for reaction optimisation. Recent work has shown that deep learning can be used to automate extraction of structured information from a variety of unstructured sources including reaction diagrams\cite{Qian2023} and experimental descriptions \cite{Guo2021}, but this approach has only been demonstrated on individual examples. Therefore, a next research project would aim to develop a algorithm that uses deep learning to  automatically extract structured information from a large number of PDF publications such as the supporting information of chemistry papers or theses. To do so, I would need to develop robust methods for both identifying relevant publications and cleaning the data once extracted.  My first demonstration would be extracting a open-source structured reaction dataset from all of the supporting information PDFs of major chemistry journals. This would offer an open-source alternative to commercial databases such as Reaxys and also build on recent work I completed with colleagues on cleaning reaction datasets (in submission).

The second direction would be collecting new data specifically for transfer learning. For the work in this thesis, my collaborators collected experimental data for both multi-task Bayesian optimisation (Part I) and distillation control (Part II), but this data collection was "narrow" focusing on a specific reaction or mixture. The work in Part III begins to illustrate the value of expanding to a wider range of molecules; in Chapter \ref{ch:ml_saft}, I showed that pretraining a neural network on a more diverse set of data from quantum mechanical simulations could improve performance on downstream prediction of unseen mixtures. However, what if the model itself could be used to automatically determine a set of data that would  improve generalization performance? Formally, this concept is called active learning, and recent work has demonstrated that, in a manner similar to the closed-loop optimisation work demonstrated in Part I and II, an active learning algorithm can choose new chemical systems to be tested experimentally that would result in better model prediction \cite{Angello2022}. The active learning algorithm samples chemistry where there is high uncertainty in model predictions, thereby expanding the domain of applicability of the model.  In a next step in this direction, I would explore if active learning can be used to improve models for prediction of reaction yield pretrained on data obtained from the literature. I would use an active learning algorithm to choose substrates that would improve the pretrained model performance if evaluated experimentally. Similarly, for thermodynamics data, I would use a multifidelity active learning algorithm to choose the new mixtures to evaluate experimentally or use quantum simulation. Such an approach would enable trading-off the cost and fidelity of simulations and experiments. 

\subsection{Translating across scales}

Process development requires translating the results of experiments at the small-scale to large-scale production. This concept has a long history in the chemical engineering literature and, generally, the approach is to define scale-independent parameters that can be measured in laboratory experiments and use these parameters in simulations to predict behavior of production processes. The activity coefficients and PCP-SAFT parameters predicted in Part III are examples of scale-independent parameters since they are intrinsic parameters of the system that do not vary across volume or amount. However, even once traditional  scale-independent parameters, there is often a difference between simulation predictions and data collected from actual processes. For example, the simulation of the distillation columns in Chapter \ref{ch:mfbo} leveraged activity coefficients measured experimentally, but the dynamic simulation still had error from experiments. 

In future work, I would explore if transfer learning can be used to improve dynamic models of chemical process simulations. In particular, I would examine whether recent work on neural state space models such as neural ordinary differential equations (ODE) \cite{Chen2018} could be pretrained on high fidelity simulations and then fine tuned on limited experimental data. Recent work has shown that hybrid neural ODEs with embedded constitutive equations such as mass balances can improve simulations of bioreactors \cite{Bangi2022}. However, this hybrid architecture only works well when the original constitutive equations are simple. I hypothesize that learning in end-to-end manner would enable learning from more complex simulations, enabling both more accuracy and significant speed-ups compared to the original simulations due to the need to do a simple forward pass through the network at each step of the differential equation solver. By first pretraining on simulation and then fine-tuning on production data, it might be possible to better translate across the gap between small scale experiments and large scale production.

% In terms of predictive thermodynamics, there are several next steps for the research programme. As noted in the conclusion section of Chapter \ref{ch:deep_gamma}, recent work has shown several ways to make models like \textit{DeepGamma} more thermodynamically consistent. My preferred method relies on a physics informed loss, yet this approach has not been trained on data with variable temperature or data from experiments. I expect that doing so would lead to state-of-the art performance for activity coefficient prediction. Another direction is including uncertainty calibration in the models. End-users of these models often want to understand whether they should be confident in the model predictions, and demonstrating strong calibration could help end-users in their decision making process.


% \subsection{More scalable models}

%  The key enabling transfer learning technology for both the reaction optimisation and controller tuning sections were multi-task GPs. While these models perform well in the low data regime, their training complexity scales with $O(N^3)$ where $N$ is the amount of data. Therefore, the Bayesian optimisations methods used in this thesis might be untenable for the larger chemistry datasets based on literature and internal company data (e.g., ORD \cite{ord}) that are becoming available. A promising research direction might be to applying methods for scaling GPs to larger datasets such as deep kernel learning\cite{Wilson2016} and inducing points \cite{Quinonero2005}. Indeed, a recent paper showed that neural kernels combined with GPs could scale to larger reaction datasets, albeit without transfer from literature data \cite{Angello2022}.

\subsection{Conclusions}

Overall, this thesis demonstrates that transfer learning is a promising approach for accelerating process development. As new data becomes available both from simulations and experiments, transfer learning will provide a basis for integrating these heterogeneous data sources to make accurate predictions of process behavior. In turn, this could enable the next generation of sustainable chemical processes to come to market faster.