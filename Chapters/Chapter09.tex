%************************************************
\chapter{Discussion}\label{ch:discussion} 
%************************************************

As the fine chemicals industry aims to become more cost efficient and environmentally friendly, optimization of processes at the early stage will become critical. In particular, flow chemistry will play a large role, but the speed of adoption has historically been slow due the challanges associated with developing robust flow chemistry processes.

\section{Thesis Contributions}

In this thesis, I aimed to address the question "Can transfer learning accelerate process development?"  Transfer learning provides a series of methods to leverage data from various sources to improve machine learning models. Much of the work presented in this thesis provides frameworks for integrating data from various sources, the often overlooked and challenging aspect of transfer learning. With these frameworks in place, I demonstrate that transfer learning can accelerate process development in several ways. While I do not focus exclusively on flow chemistry, many of the examples from reaction optimization and distillation are based on real industrial flow chemistry processes.

In part I, I show that transfer learning can reduce the number of experiments required for reaction optimization. Reaction optimization plays a significant role in early stage process development and will be increasingly important as batch processes are translated to flow chemistry. However, testing automated optimization strategies experimentally can be expensive from both a time and financial perspective.  Therefore, I first built Summit, a framework for benchmarking reaction optimization \textit{in silico}. Summit enabled robust studies of various strategies for reaction optimization, and I found that Bayesian optimization performed better than other optimization strategies across several on chemically motivated benchmarks. Subsequently, I used Summit to conduct benchmarking studies of the transfer learning technique multitask Bayesian optimization (MTBO) \cite{Swersky2013}. By leveraging data-driven benchmarks created from literature data, I demonstrated that MTBO could leverage from related experiments to reduce the number of experiments required to find reaction conditions that resulted in high yield. Furthermore, my experimental colleagues demonstrated that MTBO was effective in optimizing C-H activation reaction in a flow chemistry reactor.  Overall, this section demonstrated that value of using historical experimental data to accelerate optimization of chemical processes, particularly in early stage process development. 

In Part II, I develop approaches to automated controller tuning with a specific focus on distillation control. Distillation columns are inherently unstable unit operations and therefore require a control system for stable operation. Although advanced control methods such as model predictive control have been shown to be effective for chemical process control, simple linear feedback controllers, particularly PID controllers are still the most commonly used in the industry due to their operational simplicity. Therefore, I focus on improving PID controller tuning. I first attempted to use reinforcement learning to tune PID controllers but found that my method was both unstable and computationally inefficient. Therefore, I took a different approach that used multifidelity Bayesian optimization to combine simulation and experiments.  WRITE MORE HERE.

In part III, I explore methods for utilizing machine learning for predictive thermodynamics. Thermodynamics underlie many chemical process simulations including distillation simulations, yet acquiring thermodynamic data for new compounds experimentally is slow and expensive. I present two approaches to predictive thermodynamics. \textit{DeepGamma} is a deep learning model for activity coefficient prediction. I demonstrate the \textit{DeepGamma} can reproduce data from the thermodynamics program COSMO-RS with a 1900x speed-up, but I find that pretraining  did not improve the performance of the model. This is likely due to using a transfer learning task that was substantially different from the primary task. Furthermore, while \textit{DeepGamma} predictions are accurate, they are not thermodynamically consistent an would not be useable in a real process simulation. While work published after \textit{DeepGamma} addresses this issue for activity coefficient prediction \cite{Winter2022, SanchezMedina2023}, I decided to dedicate my remaining research time to using predictive thermodynamics in the context of an equation of state that provided thermodynamic guarantees by default. I introduced ML-SAFT, a framework for predicting the parameters of the PCP-SAFT equation of state and showed that random forests perform best on newly created dataset. However, I further demonstrated that pretraining on data from COSMO-RS simulations can improve the performance of message passing neural networks. This demonstrates that transfer learning can improve the performance of deep learning in the low data limit.

\section{Future directions}

This thesis spurred many new questions that I think should be considered in the future. The key enabling transfer learning technology for both the reaction optimization and controller tuning sections were multi-task GPs. While these models perform well in the low data regime, their training complexity scales with $O(N^3)$ where $N$ is the amount of data. Therefore, the Bayesian optimizations methods used in this thesis might be untenable for the larger chemistry datasets based on literature and internal company data (e.g., ORD \cite{ord}). A promising research direction might be to applying methods for scaling GPs to larger datasets such as deep kernel learning\cite{Wilson2016} and inducing points \cite{Quinonero2005}. Indeed, a recent paper showed that neural kernels combined with GPs could scale to larger reaction datasets, albeit without transfer from literature data.\cite{Angello2022}


In terms of predictive thermodynamics, there are several next steps for the research programme. As noted in the conclusion section of Chapter \ref{ch:deep_gamma}, recent work has shown several ways to make models like \textit{DeepGamma} more thermodynamically consistent. My preferred method relies on physics informed loss, yet this approach has not been trained on data with variable temperature or data from experiments. I expect that doing so would lead to state-of-the art performance for activity coefficient prediction. Another direction is including uncertainty calibration in the models. End-users of these models often want to understand whether they should be confident in the model predictions, and demonstrating strong calibration could help end-users in their decision making process.

Overall, this thesis demonstartes that transfer learning is a promising approach for accelerating process development. As new data becomes available both from simulations and experiments, transfer learning will provide a basis for integrating these heterogeneous data sources to make accurate predictions of process behavior. In turn, this could enable the next generation of sustainable chemical processes to come to market faster.