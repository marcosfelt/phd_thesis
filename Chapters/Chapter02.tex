%*****************************************
\chapter{Background}\label{ch:summit}
%*****************************************

\section{Transfer Learning}

% Inductive bias in machine learning

\subsection{Pretraining}

% Self-supervised tasks

% Representation learning and pretraining

\subsection{Multi-task Learning}

Multi-task learning is a form of transfer learning that builds one model to predict several related tasks. By leveraging the similarity between the tasks, the multi-task model often achieves more accurate predictions than individual models trained on each task.\cite{Simes2018} As a form of transfer learning, multi-task learning is particularly useful when the amount of data available for each task is limited.

Previously, multi-task learning has found use in several life science applications. Ramsundar and co-workers Ire the first to complete a detailed study of using multi-task deep learning for predicting the activity of molecules in various biological assays.\cite{Ramsundar2015} They found that increasing the number of tasks results in better performance in most cases, primarily due to active compounds sharing similar mechanisms across tasks. The same authors extended this work by comparing several different deep learning architectures techniques to the gold standard random forests on different pharmaceuticals datasets. They again found that multi-task learning is very effective at improving over random forests, though not always.\cite{Ramsundar2017} In contrast, Sadawi compared various types of multi-task learning in several QSAR datasets and found that random forests often outperformed deep learning models.\cite{Sadawi2019} These results suggest that performance of multi-task learning is highly dependent on the task.

More recent work has examined how multi-task learning can be used for chemical reactions. Struble et al. used a multi-task deep learning model to predict site selectivity of C-H activation reactions, resulting in a accuracy improvement compared single task models.\cite{Struble2020} The accuracy improvement of the multi-task model was attributed to several of the tasks having a small number of examples but high similarity to other tasks. In another study, a multi-task transformer model was used to predict the outcomes of carbohydrate reactions.\cite{Pesciullesi2020} The authors found that training in multi-task mode instead of sequential fine-tuning led to loIr error on the test set. 

These studies demonstrate that multi-task learning is a potentially promising method to improve the performance of models in the life sciences. However, before the work in this chapter, there was no application of multi-task learning to reaction optimization. Here, I use Bayesian optimization, which requires probabilistic models. While both random forests and deep learning models can be modified to give probabilistic results,\cite{Ling2017, Duan2020, Soleimany2021} their calibrated uncertainties are not excellent in low data regimes. In contrast, GPs, which are commonly used as probabilistic models in Bayesian optimization, offer calibrated uncertainties as a standard feature (see chapter one), making them the best choice for the application of reaction optimization. The challenge is in adapting GPs for multi-task prediction.

% \subsection{Meta-learning}

% % Model based, memory based optimization based, 

% % Transition:discuss two fields of ML applied in this thesis: BO and molecular proeprty prediction

\section{Bayesian optimization}
% Oriigns of BO

Bayesian optimization aims to solve the optimization problem.
\begin{equation}
    \max_x y(x)
\end{equation}
where $y(x)$ is the underlying function that are observed via experiments. Bayesian optimization (BO) achieves this optimization by first training a surrogate probabilistic model to represent the underlying function and, then, optimizing an acquisition function informed by this model to choose next experiments. Typically,  the probabilistic model is a Gaussian Process (GP). 

\subsection{Gaussian Processes}

A GP is a stochastic process characterized by a mean function $\mu(x)$ and covariance function $k_{\theta}(x,x')$.\cite{Rasmussen2006} The covariance function is often called a kernel, which is the term I will use henceforth.

\begin{equation}
    f(x)= \mathcal{GP}(\mu(x), k_{\theta}(x, x'))
\end{equation}

$\theta$ are referred to as the hyperparameters of the GP and are varied to train the GP. Given a finite set of $N$ inputs $\mathbf X = \{\mathbf x_1, \mathbf x_2, \dots, \mathbf x_N \ \vert x_i \in \mathbb R^m \}$ that correspond with outputs $\mathbf y = \{y_1, y_2, \dots y_N \vert  y_i \in \mathbb R \}$, the GP is a multivariate Gaussian distribution:

\begin{equation}
    f(\mathbf X) \sim \mathcal N(\mu_{\theta}(\mathbf X), k_{\theta}(\mathbf X, \mathbf X'))
\end{equation}

The mean function and kernel act as a prior on the GP.  $\mu_{\theta}(x)$ is usually set to zero because the kernel  $k_{\theta}(x, x')$ can expressively represent any arbitrary function. In this work, I use the Mat√©rn 5/2 kernel, with hyperparameters $\theta=\{\sigma,\mathbf L \}$. $\sigma \in \mathbb R$ is the scaling hyperparameter and $\mathbf L \in \mathbb R^m$ is a lengthscale that indicates the significance of each input feature:
\begin{equation}
    k_{\theta}(x, x') = \sigma^2 \biggl(1 + \sqrt{5}d_{\theta}(x,x')+\frac{5}{3}d_{\theta}(x,x')^2\biggr)\exp\biggl(-\sqrt{5}d_{\theta}(x,x') \biggr)
\end{equation}
$d_{\theta}(x,x')$ is the euclidean distance weighted by the lengthscale.
\begin{equation}
    d_{\theta}(x,x')=\biggl\lVert \frac{x-x'}{L} \biggr\rVert_2
\end{equation}
Inference on the GP is done by calculating the posterior of the GP. The posterior of the GP is also a Gaussian distribution:

\begin{equation}
     \tilde f(\mathbf X) \sim \mathcal N(\tilde \mu(\mathbf X), \tilde \sigma_{\theta}(\mathbf X, \mathbf X'))
\end{equation}

\begin{equation}
    \tilde \mu_{\theta}(x) = k_{\theta}(x, \mathbf X)k_{\theta}(\mathbf X, \mathbf X')^{-1} \mathbf y
\end{equation}

\begin{equation}
    \tilde k_{\theta}(x,x') = k_{\theta}( x, x')-k_{\theta}(x, \mathbf X) k_{\theta}(\mathbf X, \mathbf X)^{-1}k_{\theta}(\mathbf X, x)
\end{equation}
where $\tilde \sigma_{\theta}(x)$ are the diagonals of the covariance matrix calculated using $\tilde k_{\theta}(x, x')$.
To train the GP, the log likelihood is maximized, which is the probability that the model predicts the training outputs given the inputs and hyperparameters. The log likelihood avoids overfitting by trading off accuracy of fit to the training data and complexity of the model.
\begin{equation}
    \log p(y \vert X, \theta) = -\underbrace{\frac{1}{2}(y-\tilde \mu_{\theta}(\mathbf X))^T k_{\theta}(\mathbf X, \mathbf X)^{-1}(y- \tilde\mu_{\theta}(\mathbf X)) }_{\text{Data  fit}}- \underbrace{\frac{1}{2} \log{\vert \tilde k_{\theta}(\mathbf X, \mathbf X) \vert} - \frac{d}{2}\log{2 \pi}}_{\text{Complexity penalty}}
\end{equation}

\subsection{Acquisition Functions}

The most commonly used acquisition function is the expected improvement (EI). In BO with EI as an acquisition function, the aim is to choose the point that is expected to improve the most upon the existing best observed point $y^* \geq \hat y(x_i) \forall i \in (1, \dots, t)$  where  $t$ is the number of observations thus far. Therefore, I create an improvement function $I(x)$ describing the improvement of the posterior of the GP over the best observed point. If there is no improvement, $I(x)=0$.
\begin{equation}
    I(x) = \max(\tilde f_{\theta}(x) -y^*, 0)
\end{equation}
For EI, I want the expectation of the improvement:
\begin{equation}
    EI(x) = \mathbb E_{y}[I(x)]
\end{equation}
After some manipulations, the following closed form for the expected improvement is found:
\begin{equation}
    EI(x) =(\tilde \mu_{\theta}(x)-\hat y^*)\Phi(Z^*) + \tilde \sigma_{\theta}(x) \phi(Z^*)
\end{equation}
where $Z^*= \frac{y^*-\tilde\mu_{\theta}(x)}{\tilde \sigma_{\theta}(x)}$.  I then solve the following optimization problem to select the next experiment $x_{next}$.
\begin{equation}
    x_{\text{next}} = \text{argmax}_{x} EI(x)
\end{equation}
Since EI has a closed analytical for the expectation and derivative, gradient based second-order optimization algorithms are typically used. 

While EI is a good baseline acquisition function, it can be overly exploitative, especially in the presence of noisy or inaccurate predictions from the GP.  Noisy EI (NEI) improves upon this by sampling the expectation of the acquisition function and the posterior probability, which reduces uncertainty in the optimum.\cite{Letham2019}. A robust and efficient impelementation of the NEI that uses a full Monte-Carlo treatment is available in the software package BOtorch \cite{Balandat2020}:

\begin{equation}
qNEI(x)= E[(\max \xi  - \max \xi_{obs} )_+]
\end{equation}

where $\xi_{obs} f(x)$ and $\xi_{obs}\sim f(x)$ are samples from the posterior of the GP.

\subsection{Transfer learning for Bayesian optimization}

One of the potential ways to accelerate Bayesian optimization is multi-task Bayesian optimization (MTBO). MTBO was originally developed to speed up hyperparameter tuning of machine learning models (e.g., choosing learning rates and batch sizes to maximize model accuracy) \cite{Swersky2013}. Using only the hyperparameters and resulting model accuracy scores of a previously trained machine learning model (which I call Task A),  MTBO decreased by up to 50\% the the number of experiments needed to find optimal hyperparameters for a new machine learning model (which I call Task B).  The data from Task A helped the model better predict Task B, even with only a few experiments for Task B.

Multi-task Bayesian optimization (MTBO) uses a multi-task probabilistic model inside a BO framework.  Swersky et al. were the first to demonstrate that multi-task BO can be effectively used to accelerate BO for hyperparameter tuning of machine learning models \cite{Swersky2013}. They demonstrated that using the results of hyperparameter tuning on one dataset could assist in tuning another with multi-task GPs. In many cases, multi-task BO could achieve 40-50\% improvements in the test accuracy of models with significantly feIr training runs.  

One of the challenges faced in applying MTBO to big data uses cases has been its lack of scalability. This in mainly due to the $O(n^3)$ cost of using GPs with exact inference. Several different approaches have been taken to solving this problem including training a vanilla neural network on the tasks and feeding the output to a Bayesian linear regression model \cite{Perrone2018} and using the auxiliary tasks to create a learned feature representation in a compressed space \cite{Hakhamaneshi2021}. These methods have retained the performance of multi-task BO while minimizing the computational cost of its deployment. However, in our case, all datasets have less than 1000 points and in many cases, less than 100. This makes multi-task GPs with exact inference more tractable.

Another direction has been to apply the machinery of multi-task GPs to multifidelity BO, which aims to leverage data from a cheap but less accurate data source to help with optimizing a more expensive function. This technique has been applied in a wide range of scenarios including parameter estimation of physics models using data from multiple experiments and simulations;\cite{Perdikaris2016} optimizing battery electrode structure using data from both cheap and expensive multitscale differential equation models (Pan 2017), and optimizing composition of alloys using a mix of DFT fidelities (Tran 2020). I foresee that, given these results, our approach will be able to be extended to combine data from simulations and experiments to rapidly optimize processes.

\section{Molecular Property Prediction}

\subsection{Molecular Representations}

% Circular fingerprints

% Graph representations

\subsection{Model Architectures}

% Random forest

% Neural networks 